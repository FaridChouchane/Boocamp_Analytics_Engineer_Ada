# â„ï¸ Snowflake â€” Chapitre 4 : Charger les DonnÃ©es

> **Niveau** : DÃ©butant â†’ IntermÃ©diaire | **DurÃ©e estimÃ©e** : 3-4h  
> **PrÃ©requis** : Bases SQL, compte Snowflake actif, warehouse `COMPUTE_WH` disponible  
> **Projet fil rouge** : Pipeline `health_app` â€” ingestion du fichier `HealthApp_2k.log` depuis GCS vers Snowflake

---

## ğŸ“‹ Sommaire

1. [Vue d'ensemble & Architecture](#1-vue-densemble--architecture)
2. [CrÃ©er la structure de donnÃ©es](#2-crÃ©er-la-structure-de-donnÃ©es)
3. [Les Stages : zone de transit](#3-les-stages--zone-de-transit)
4. [File Formats : dÃ©crire vos fichiers](#4-file-formats--dÃ©crire-vos-fichiers)
5. [COPY INTO : chargement en masse](#5-copy-into--chargement-en-masse)
6. [Workflow complet GCS â†’ Snowflake](#6-workflow-complet-gcs--snowflake)
7. [Chargement INSERT (petits volumes)](#7-chargement-insert-petits-volumes)
8. [Snowpipe : ingestion continue](#8-snowpipe--ingestion-continue)
9. [PUT & GET : CLI SnowSQL](#9-put--get--cli-snowsql)
10. [Formats de fichiers supportÃ©s](#10-formats-de-fichiers-supportÃ©s)
11. [Cas pratiques complets](#11-cas-pratiques-complets)
12. [ğŸ”´ Dictionnaire des erreurs communes](#12--dictionnaire-des-erreurs-communes)
13. [Patterns rÃ©utilisables](#13-patterns-rÃ©utilisables)
14. [âœ… Checklist & Bonnes pratiques](#14--checklist--bonnes-pratiques)
15. [Glossaire complet](#15-glossaire-complet)

---

## 1. Vue d'ensemble & Architecture

### Objectifs de cette section
- Comprendre les 3 mÃ©canismes de chargement et savoir lequel choisir
- Visualiser l'architecture globale Source â†’ Stage â†’ Table
- Identifier les composants clÃ©s (Storage Integration, Stage, File Format)

---

### 1.1 Les 3 grandes mÃ©thodes

| MÃ©thode | Cas d'usage | Latence | ComplexitÃ© | CoÃ»t |
|---|---|---|---|---|
| **COPY INTO** (Batch) | Fichiers en masse : CSV, JSON, Parquetâ€¦ | Minutes | â­ Faible | WH classique |
| **Snowpipe** (Near-realtime) | Ingestion continue dÃ©clenchÃ©e par Ã©vÃ©nement | Secondes | â­â­ Moyenne | Serverless |
| **PUT + COPY** (CLI) | Dev/test, uploads manuels ponctuels | Variable | â­ Faible | WH classique |
| **INSERT** | Petits volumes, donnÃ©es statiques | ImmÃ©diat | â­ Faible | WH classique |

> ğŸ’¡ **RÃ¨gle de dÃ©cision** : volume > 1 MB â†’ COPY INTO. Besoin de rÃ©activitÃ© < 1 min â†’ Snowpipe. Test rapide ou quelques dizaines de lignes â†’ INSERT.

---

### 1.2 Architecture globale

```mermaid
flowchart LR
    subgraph SOURCES["ğŸ—‚ï¸ Sources de donnÃ©es"]
        A["ğŸ“„ Fichiers locaux\n.csv .json .parquet"]
        B["â˜ï¸ GCS Bucket\ngcs://mon-bucket/"]
        C["â˜ï¸ S3 Bucket\ns3://mon-bucket/"]
        D["â˜ï¸ Azure Blob\nazure://container/"]
    end

    subgraph TRANSIT["ğŸš‰ Zone de Transit â€” Stages"]
        E["Internal Stage\n@~ / @%table / @named\nStockage Snowflake"]
        F["External Stage\n@gcs_stage / @s3_stage\nPointeur vers le cloud"]
    end

    subgraph SNOWFLAKE["â„ï¸ Snowflake"]
        G["ğŸ“„ File Format\nDÃ©crit CSV/JSON/Parquet"]
        H["ğŸ“¥ COPY INTO\nCharge en masse"]
        I["ğŸ—ƒï¸ Table Snowflake\nDonnÃ©es disponibles"]
    end

    A -->|"PUT via SnowSQL CLI"| E
    B -->|"Storage Integration GCS"| F
    C -->|"Storage Integration S3"| F
    D -->|"Storage Integration Azure"| F
    E --> G
    F --> G
    G --> H
    H --> I

    style SOURCES fill:#1A3A5C,color:#fff
    style TRANSIT fill:#2E75B6,color:#fff
    style SNOWFLAKE fill:#29B5E8,color:#fff
```

---

### 1.3 Flux de chargement â€” schÃ©ma mental

```mermaid
flowchart LR
    A["â˜ï¸ Cloud Storage\nGCS / S3 / Azure\nâ‘  DonnÃ©es sources"] --> B["ğŸ” Storage Integration\nâ‘¡ Autorise Snowflake\nÃ  accÃ©der au bucket"]
    B --> C["ğŸš‰ External Stage\nâ‘¢ Pointeur vers\nle bucket cloud"]
    C --> D["ğŸ“„ File Format\nâ‘£ DÃ©crit comment\nlire les fichiers"]
    D --> E["ğŸ“¥ COPY INTO\nâ‘¤ Lit, parse\net insÃ¨re"]
    E --> F["ğŸ—ƒï¸ Table Snowflake\nâ‘¥ DonnÃ©es prÃªtes\npour l'analyse"]

    style A fill:#4285F4,color:#fff
    style B fill:#EA4335,color:#fff
    style C fill:#FBBC04,color:#000
    style D fill:#34A853,color:#fff
    style E fill:#29B5E8,color:#fff
    style F fill:#1A3A5C,color:#fff
```

---

### 1.4 Cycle de vie d'un fichier chargÃ©

```mermaid
stateDiagram-v2
    [*] --> STAGE : Fichier dÃ©posÃ© dans le Stage\n(PUT, upload GCS/S3, Snowpipe)

    STAGE --> VALIDATION : COPY INTO avec\nVALIDATION_MODE
    VALIDATION --> STAGE : Erreurs trouvÃ©es\nâ†’ corriger le fichier

    STAGE --> LOADING : COPY INTO sans\nVALIDATION_MODE
    LOADING --> LOADED : SuccÃ¨s\n(metadata enregistrÃ©e)
    LOADING --> FAILED : Erreur\n(ON_ERROR dÃ©termine\nla suite)

    FAILED --> SKIPPED : ON_ERROR = SKIP_FILE\nou CONTINUE
    FAILED --> ABORTED : ON_ERROR = ABORT_STATEMENT

    LOADED --> PURGED : PURGE = TRUE\n(fichier supprimÃ© du stage)
    LOADED --> STAGE : Fichier conservÃ©\n(PURGE = FALSE)

    LOADED --> DEDUP : Fichier dÃ©jÃ  chargÃ© ?\nMÃ©tadata Snowflake\nbloque le re-chargement
    DEDUP --> SKIPPED : FORCE = FALSE\n(comportement par dÃ©faut)
    DEDUP --> LOADED : FORCE = TRUE\n(âš ï¸ risque de doublons)
```

ğŸ“– **Doc officielle** : [Data Loading Overview](https://docs.snowflake.com/fr/user-guide/data-load-overview)

---

## 2. CrÃ©er la Structure de DonnÃ©es

### Objectifs de cette section
- CrÃ©er la hiÃ©rarchie Database â†’ Schema â†’ Table
- Choisir les bons types de donnÃ©es Snowflake
- Comprendre `CREATE OR REPLACE` vs `CREATE IF NOT EXISTS`

---

### 2.1 CrÃ©er Database, Schema, Table

```sql
-- â‘  CrÃ©er la base de donnÃ©es
-- IF NOT EXISTS : ne plante pas si elle existe dÃ©jÃ  (idempotent)
CREATE DATABASE IF NOT EXISTS HEALTH_APP
    COMMENT = 'Base principale application santÃ©';

-- â‘¡ CrÃ©er le schÃ©ma de donnÃ©es brutes
CREATE SCHEMA IF NOT EXISTS HEALTH_APP.RAW
    COMMENT = 'DonnÃ©es brutes avant transformation â€” ne jamais modifier directement';

-- â‘¢ SÃ©lectionner le contexte de travail
USE DATABASE HEALTH_APP;
USE SCHEMA RAW;

-- â‘£ CrÃ©er la table de destination
-- OR REPLACE : recrÃ©e la table si elle existe (attention : supprime les donnÃ©es existantes)
CREATE OR REPLACE TABLE RAW_EVENTS (
    event_id        INTEGER,            -- identifiant unique de l'event
    event_timestamp TIMESTAMP,          -- horodatage de l'event (UTC recommandÃ©)
    process_name    VARCHAR(100),       -- nom du process applicatif (ex: 'Step_LSC')
    process_id      INTEGER,            -- PID du process
    message         VARCHAR(16777216)   -- message brut (max = 16MB dans Snowflake)
);
```

> âš ï¸ **`CREATE OR REPLACE` vs `CREATE IF NOT EXISTS`** : `OR REPLACE` recrÃ©e la table et **supprime toutes les donnÃ©es**. En production, prÃ©fÃ©rer `CREATE TABLE IF NOT EXISTS` ou `ALTER TABLE` pour ajouter des colonnes.

---

### 2.2 Types de donnÃ©es Snowflake â€” rÃ©fÃ©rence complÃ¨te

| Type | Alias courants | Taille max | Quand l'utiliser |
|---|---|---|---|
| `NUMBER(p, s)` | `INTEGER`, `BIGINT`, `DECIMAL` | 38 chiffres | Entiers, montants, IDs |
| `FLOAT` | `DOUBLE`, `REAL` | ~15 chiffres sig. | Mesures, ratios, coordonnÃ©es GPS |
| `VARCHAR(n)` | `STRING`, `TEXT` | 16 777 216 chars | Texte, noms, codes |
| `DATE` | â€” | â€” | Date seule sans heure |
| `TIMESTAMP_NTZ` | `TIMESTAMP` | â€” | Date+heure **sans** fuseau |
| `TIMESTAMP_TZ` | â€” | â€” | Date+heure **avec** fuseau (logs prod) |
| `BOOLEAN` | â€” | â€” | Flags, indicateurs binaires |
| `VARIANT` | â€” | 16 MB | JSON, AVRO, XML semi-structurÃ© |
| `ARRAY` | â€” | 16 MB | Listes JSON |
| `OBJECT` | â€” | 16 MB | Objets JSON avec clÃ©s nommÃ©es |

> ğŸ’¡ **Conseil** : pour les logs applicatifs, utiliser `TIMESTAMP_TZ` (conserve le fuseau d'origine) ou `TIMESTAMP_NTZ` + convention UTC. Ne pas utiliser `TIMESTAMP_LTZ` (fuseau local du compte Snowflake â€” source de bugs).

```mermaid
mindmap
  root((Types de donnÃ©es\nSnowflake))
    NumÃ©riques
      NUMBER p s
        entiers dÃ©cimaux
      FLOAT DOUBLE
        mesures ratios
    Texte
      VARCHAR n
        jusqu Ã  16MB
      CHAR n
        longueur fixe
    Temporels
      DATE
        sans heure
      TIMESTAMP_NTZ
        sans fuseau UTC
      TIMESTAMP_TZ
        avec fuseau prod
    Semi-structurÃ©s
      VARIANT
        JSON AVRO XML
      ARRAY
        listes JSON
      OBJECT
        objets JSON
    Autres
      BOOLEAN
        TRUE FALSE
      BINARY
        donnÃ©es binaires
```

ğŸ“– Docs officielles :
- [CREATE DATABASE](https://docs.snowflake.com/fr/sql-reference/sql/create-database)
- [CREATE SCHEMA](https://docs.snowflake.com/fr/sql-reference/sql/create-schema)
- [CREATE TABLE](https://docs.snowflake.com/fr/sql-reference/sql/create-table)
- [Types de donnÃ©es](https://docs.snowflake.com/fr/sql-reference-data-types)

---

## 3. Les Stages : Zone de Transit

### Objectifs de cette section
- Comprendre la diffÃ©rence entre stage interne et externe
- CrÃ©er un stage externe GCS avec Storage Integration
- Utiliser LIST, REMOVE et inspecter le contenu brut d'un stage

---

### 3.1 Qu'est-ce qu'un Stage ?

Un **Stage** est une zone de transit oÃ¹ les fichiers sont dÃ©posÃ©s avant d'Ãªtre chargÃ©s dans une table. Snowflake ne stocke pas les donnÃ©es dans les stages externes â€” il pointe vers le cloud.

```mermaid
graph LR
    subgraph "Stages Internes\n(stockage Snowflake)"
        A["@~\nUser Stage\nPersonnel, par utilisateur"]
        B["@%NOM_TABLE\nTable Stage\nLiÃ© Ã  une table prÃ©cise"]
        C["@mon_stage\nNamed Stage\nPartageable entre rÃ´les"]
    end

    subgraph "Stages Externes\n(stockage cloud)"
        D["@gcs_stage\nGCS Bucket\ngcs://ada_snowflake/"]
        E["@s3_stage\nS3 Bucket\ns3://mon-bucket/"]
        F["@az_stage\nAzure Blob\nazure://container/"]
    end

    PUT["ğŸ“¤ PUT\n(SnowSQL CLI\nuniquement)"] --> A
    PUT --> C
    D -->|"Storage Integration\n= droits IAM"| GCS["â˜ï¸ GCS"]
    E -->|"Storage Integration\n= droits IAM"| S3["â˜ï¸ AWS S3"]
    F -->|"Storage Integration\n= droits IAM"| AZ["â˜ï¸ Azure"]

    style A fill:#2E75B6,color:#fff
    style B fill:#2E75B6,color:#fff
    style C fill:#2E75B6,color:#fff
    style D fill:#4285F4,color:#fff
    style E fill:#FF9900,color:#fff
    style F fill:#0089D6,color:#fff
```

---

### 3.2 Comparaison des Stages

| Type | Syntaxe | Stockage | Limites | Usage recommandÃ© |
|---|---|---|---|---|
| User Stage | `@~` | Interne Snowflake | Personnel, non partageable | Tests rapides perso |
| Table Stage | `@%NOM_TABLE` | Interne Snowflake | LiÃ© Ã  1 seule table | Chargement ponctuel dans 1 table |
| Named Stage (interne) | `@mon_stage` | Interne Snowflake | Quota de stockage Snowflake | Partage entre Ã©quipes, pipelines internes |
| Named Stage (externe) | `@gcs_stage` | GCS / S3 / Azure | DÃ©pend du cloud | **Production, gros volumes, Ã©quipes** |

---

### 3.3 CrÃ©er un Stage Interne NommÃ© â€” syntaxe annotÃ©e

```sql
-- Stage interne le plus simple
CREATE OR REPLACE STAGE mon_stage_interne
    COMMENT = 'Stage interne pour chargement CSV de test';

-- Stage interne avec File Format intÃ©grÃ©
-- Avantage : pas besoin de rÃ©pÃ©ter le format dans chaque COPY INTO
CREATE OR REPLACE STAGE mon_stage_csv
    FILE_FORMAT = (
        TYPE            = 'CSV'
        FIELD_DELIMITER = ','
        SKIP_HEADER     = 1
    )
    COMMENT = 'Stage CSV avec en-tÃªte ignorÃ©';

-- Lister les fichiers prÃ©sents dans le stage
LIST @mon_stage_interne;
-- â†’ Retourne : name, size, md5, last_modified

-- Inspecter le contenu brut d'un fichier (sans charger)
SELECT $1, $2, $3, $4, $5
FROM @mon_stage_interne/mon_fichier.csv
    (FILE_FORMAT => (TYPE = 'CSV'))
LIMIT 10;
-- $1, $2... = colonnes positionnelles du fichier brut

-- Supprimer un fichier spÃ©cifique du stage
REMOVE @mon_stage_interne/mon_fichier.csv;

-- Vider entiÃ¨rement un stage
REMOVE @mon_stage_interne;
```

---

### 3.4 CrÃ©er un Stage Externe GCS â€” workflow complet

La crÃ©ation d'un stage externe nÃ©cessite d'abord une **Storage Integration** â€” objet Snowflake qui contient les credentials IAM sÃ©curisÃ©s pour accÃ©der au bucket.

```mermaid
sequenceDiagram
    actor Admin as ğŸ‘¤ Admin Snowflake
    participant SF as â„ï¸ Snowflake
    participant IAM as ğŸ” GCS IAM
    participant GCS as â˜ï¸ GCS Bucket

    Admin->>SF: â‘  CREATE STORAGE INTEGRATION gcs_int_ada
    SF-->>Admin: â‘¡ Retourne STORAGE_GCP_SERVICE_ACCOUNT\n(email Snowflake gÃ©nÃ©rÃ©)
    Admin->>IAM: â‘¢ Ajouter l'email comme\nStorage Object Viewer sur le bucket
    IAM-->>Admin: âœ… Permission accordÃ©e

    Admin->>SF: â‘£ CREATE STAGE gcs_stage (URL = gcs://...)
    Admin->>SF: â‘¤ LIST @gcs_stage
    SF->>GCS: VÃ©rifie l'accÃ¨s
    GCS-->>SF: Liste des fichiers
    SF-->>Admin: âœ… Fichiers visibles â†’ stage opÃ©rationnel
```

```sql
-- â‘  CrÃ©er la Storage Integration (une seule fois par compte/bucket)
-- NÃ©cessite le rÃ´le ACCOUNTADMIN
CREATE OR REPLACE STORAGE INTEGRATION gcs_int_ada
    TYPE                      = EXTERNAL_STAGE   -- type obligatoire
    STORAGE_PROVIDER          = GCS              -- fournisseur cloud
    ENABLED                   = TRUE             -- activer immÃ©diatement
    STORAGE_ALLOWED_LOCATIONS = ('gcs://ada_snowflake/');  -- bucket(s) autorisÃ©(s)

-- â‘¡ RÃ©cupÃ©rer l'email du service account Snowflake
-- â†’ Copier la valeur de STORAGE_GCP_SERVICE_ACCOUNT
DESC INTEGRATION gcs_int_ada;
-- Exemple : abc123xyz@gcpserviceaccount.iam.gserviceaccount.com

-- â‘¢ (Dans GCS Console)
-- GCS > Votre Bucket > Permissions > + Grant Access
-- Principal : coller l'email Snowflake
-- Role : Storage Object Viewer (lecture seule) ou Storage Object Admin (lecture + Ã©criture)

-- â‘£ CrÃ©er le stage externe liÃ© Ã  l'intÃ©gration
CREATE OR REPLACE STAGE gcs_stage
    URL                 = 'gcs://ada_snowflake/'   -- bucket + chemin (dossier optionnel)
    STORAGE_INTEGRATION = gcs_int_ada             -- rÃ©fÃ©rence l'intÃ©gration crÃ©Ã©e ci-dessus
    FILE_FORMAT         = csv_file                -- format par dÃ©faut du stage
    COMMENT             = 'Stage GCS production â€” projet health_app';

-- â‘¤ Tester : lister les fichiers du bucket
LIST @gcs_stage;
-- Si erreur Access Denied â†’ vÃ©rifier Ã©tape â‘¢
```

> âš ï¸ **Bonne pratique sÃ©curitÃ©** : ne jamais utiliser de clÃ©s d'accÃ¨s (`AWS_KEY_ID` / `AWS_SECRET_KEY`) directement dans le stage. Toujours passer par une Storage Integration â€” les credentials sont gÃ©rÃ©s par Snowflake, pas exposÃ©s dans le SQL.

ğŸ“– [CREATE STAGE](https://docs.snowflake.com/fr/sql-reference/sql/create-stage) | [CREATE STORAGE INTEGRATION](https://docs.snowflake.com/fr/sql-reference/sql/create-storage-integration) | [Config GCS](https://docs.snowflake.com/fr/user-guide/data-load-gcs-config)

---

## 4. File Formats : DÃ©crire vos Fichiers

### Objectifs de cette section
- CrÃ©er des File Formats nommÃ©s et rÃ©utilisables
- MaÃ®triser les options clÃ©s pour CSV, JSON, Parquet
- Comprendre pourquoi dÃ©finir un format nommÃ© plutÃ´t qu'inline

---

### 4.1 Pourquoi un File Format nommÃ© ?

```mermaid
graph TD
    subgraph "âŒ Format inline â€” rÃ©pÃ©titif et fragile"
        A1["COPY INTO table_1\nFILE_FORMAT = (TYPE='CSV'\nFIELD_DELIMITER=','\nSKIP_HEADER=1\nNULL_IF=('NULL',''))"]
        A2["COPY INTO table_2\nFILE_FORMAT = (TYPE='CSV'\nFIELD_DELIMITER=','\nSKIP_HEADER=1\nNULL_IF=('NULL',''))"]
        A3["Si sÃ©parateur change â†’ modifier\ntous les COPY INTO un par un âŒ"]
    end

    subgraph "âœ… Format nommÃ© â€” rÃ©utilisable et maintenable"
        B1["CREATE FILE FORMAT csv_file\nTYPE='CSV' FIELD_DELIMITER=','\nSKIP_HEADER=1 NULL_IF=(...)"]
        B2["COPY INTO table_1\nFILE_FORMAT=(FORMAT_NAME='csv_file')"]
        B3["COPY INTO table_2\nFILE_FORMAT=(FORMAT_NAME='csv_file')"]
        B4["Si sÃ©parateur change â†’ modifier\n1 seul objet csv_file âœ…"]
    end

    style A3 fill:#e17055,color:#fff
    style B4 fill:#00b894,color:#fff
```

---

### 4.2 Format CSV â€” options complÃ¨tes annotÃ©es

```sql
CREATE OR REPLACE FILE FORMAT csv_file
    TYPE                        = CSV
    FIELD_DELIMITER             = ','       -- â‘  sÃ©parateur de colonnes (dÃ©faut: virgule)
    RECORD_DELIMITER            = '\n'      -- â‘¡ fin de ligne (dÃ©faut: \n)
    SKIP_HEADER                 = 1         -- â‘¢ nb de lignes d'en-tÃªte Ã  ignorer
    FIELD_OPTIONALLY_ENCLOSED_BY = '"'      -- â‘£ guillemets autour des champs texte
    NULL_IF                     = ('NULL', 'null', '', '\\N')  -- â‘¤ valeurs â†’ NULL Snowflake
    EMPTY_FIELD_AS_NULL         = TRUE      -- â‘¥ champ vide = NULL (sans le mettre dans NULL_IF)
    TRIM_SPACE                  = TRUE      -- â‘¦ supprimer espaces dÃ©but/fin de chaque champ
    DATE_FORMAT                 = 'YYYY-MM-DD'          -- â‘§ format attendu pour les dates
    TIMESTAMP_FORMAT            = 'YYYY-MM-DD HH24:MI:SS'  -- â‘¨ format pour les timestamps
    ENCODING                    = 'UTF-8'  -- â‘© encodage (UTF-8 recommandÃ©, ou ISO-8859-1)
    COMMENT                     = 'Format CSV standard pour exports health_app';
```

**Options `FIELD_DELIMITER` courantes :**

| Valeur | SÃ©parateur | Usage |
|---|---|---|
| `','` | Virgule | CSV standard |
| `';'` | Point-virgule | Exports Excel France |
| `'\t'` | Tabulation | TSV (Tab-Separated Values) |
| `'|'` | Pipe | EDI, exports legacy |

---

### 4.3 Format JSON

```sql
CREATE OR REPLACE FILE FORMAT fmt_json
    TYPE               = JSON
    STRIP_OUTER_ARRAY  = TRUE    -- â‘  [{...},{...}] â†’ lignes sÃ©parÃ©es (ESSENTIEL pour tableaux JSON)
    STRIP_NULL_VALUES  = FALSE   -- â‘¡ conserver les nulls JSON dans VARIANT
    IGNORE_UTF8_ERRORS = FALSE   -- â‘¢ TRUE = ignorer les caractÃ¨res invalides (mode permissif)
    ALLOW_DUPLICATE    = FALSE   -- â‘£ FALSE = rejeter les clÃ©s JSON dupliquÃ©es
    COMMENT            = 'Format JSON pour events streamÃ©s';

-- Charger JSON dans une colonne VARIANT
CREATE OR REPLACE TABLE raw_events_json (
    data      VARIANT,                           -- stocke le JSON entier
    loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
);

COPY INTO raw_events_json (data)
FROM (SELECT $1 FROM @gcs_stage/events/)
FILE_FORMAT = (FORMAT_NAME = 'fmt_json');

-- RequÃªter le JSON avec la notation : (colon)
SELECT
    data:event_id::INTEGER      AS event_id,
    data:process_name::STRING   AS process_name,
    data:timestamp::TIMESTAMP   AS ts,
    data:properties:cpu_usage   AS cpu_usage   -- accÃ¨s imbriquÃ©
FROM raw_events_json
LIMIT 10;
```

> ğŸ’¡ **`STRIP_OUTER_ARRAY = TRUE`** : si votre fichier contient `[{...}, {...}]` (tableau JSON), cette option dÃ©structure le tableau en lignes sÃ©parÃ©es. Sans elle, tout le tableau est mis dans une seule cellule VARIANT.

---

### 4.4 Format Parquet

```sql
CREATE OR REPLACE FILE FORMAT fmt_parquet
    TYPE               = PARQUET
    SNAPPY_COMPRESSION = TRUE    -- compression Snappy (dÃ©faut Parquet, trÃ¨s efficace)
    BINARY_AS_TEXT     = FALSE   -- FALSE = conserver les binaires tels quels
    COMMENT            = 'Format Parquet pour Data Lake';

-- Inspecter la structure d'un fichier Parquet avant chargement
SELECT $1
FROM @gcs_stage/data.parquet
    (FILE_FORMAT => (TYPE = 'PARQUET'))
LIMIT 5;
-- â†’ Retourne des lignes VARIANT avec la structure du fichier

-- Charger Parquet avec mapping automatique des colonnes
COPY INTO ma_table
FROM @gcs_stage/data.parquet
FILE_FORMAT        = (TYPE = 'PARQUET')
MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE;  -- mappe les colonnes par nom (pas par position)
```

ğŸ“– [CREATE FILE FORMAT](https://docs.snowflake.com/fr/sql-reference/sql/create-file-format) â€” toutes les options par format

---

## 5. COPY INTO : Chargement en Masse

### Objectifs de cette section
- MaÃ®triser la syntaxe complÃ¨te de COPY INTO
- Choisir la bonne option `ON_ERROR`
- Utiliser `VALIDATION_MODE` avant de charger
- Transformer les donnÃ©es Ã  la volÃ©e pendant le chargement
- Interroger l'historique via `COPY_HISTORY`

---

### 5.1 Syntaxe complÃ¨te annotÃ©e

```sql
COPY INTO ma_table                              -- â‘  table de destination
FROM @mon_stage/dossier/fichier.csv             -- â‘¡ source (stage + chemin optionnel)
FILE_FORMAT   = (FORMAT_NAME = 'csv_file')      -- â‘¢ format nommÃ© ou inline
FILES         = ('file1.csv', 'file2.csv')      -- â‘£ fichiers spÃ©cifiques (optionnel)
PATTERN       = '.*2024.*\.csv'                 -- â‘¤ pattern regex (alternative Ã  FILES)
ON_ERROR      = 'CONTINUE'                      -- â‘¥ comportement en cas d'erreur
PURGE         = FALSE                           -- â‘¦ TRUE = supprimer le fichier aprÃ¨s chargement
FORCE         = FALSE                           -- â‘§ TRUE = re-charger mÃªme si dÃ©jÃ  traitÃ©
TRUNCATECOLUMNS = FALSE                         -- â‘¨ TRUE = tronquer si valeur > taille colonne
LOAD_UNCERTAIN_FILES = FALSE;                   -- â‘© TRUE = tenter les fichiers de statut incertain
```

> âš ï¸ **`FORCE = TRUE`** dÃ©sactive la dÃ©duplication native de Snowflake. Chaque COPY INTO avec FORCE=TRUE peut crÃ©er des **doublons**. RÃ©server Ã  la migration initiale ou aux tests.

---

### 5.2 Options ON_ERROR â€” comportement et choix

```mermaid
flowchart TD
    A["Ligne invalide dÃ©tectÃ©e\npendant COPY INTO"] --> B{ON_ERROR\nquelle valeur ?}

    B -->|"ABORT_STATEMENT\n(dÃ©faut)"| C["ğŸ›‘ Stoppe TOUT\nRollback de toutes\nles lignes du fichier\n0 ligne chargÃ©e"]
    B -->|"CONTINUE"| D["â© Ignore cette ligne\nCharge toutes les\nautres lignes valides"]
    B -->|"SKIP_FILE"| E["ğŸ“ Ignore tout le fichier\ncourant si 1 erreur\nPasse au fichier suivant"]
    B -->|"SKIP_FILE_3"| F["ğŸ“ Ignore le fichier\nsi plus de 3 erreurs\n(nombre configurable)"]
    B -->|"SKIP_FILE_10%"| G["ğŸ“ Ignore le fichier\nsi plus de 10%\nde lignes en erreur"]

    style C fill:#C0392B,color:#fff
    style D fill:#27AE60,color:#fff
    style E fill:#E67E22,color:#fff
    style F fill:#F39C12,color:#fff
    style G fill:#F39C12,color:#fff
```

| Option | Comportement | Quand l'utiliser |
|---|---|---|
| `ABORT_STATEMENT` (dÃ©faut) | Stoppe tout, rollback complet | DonnÃ©es critiques, 0 tolÃ©rance erreur |
| `CONTINUE` | Charge les lignes valides, ignore les mauvaises | Logs, donnÃ©es semi-fiables |
| `SKIP_FILE` | Ignore tout le fichier Ã  la premiÃ¨re erreur | Fichiers atomiques (tout ou rien) |
| `SKIP_FILE_n` | Ignore si plus de n erreurs dans le fichier | Seuil d'erreur connu et acceptable |
| `SKIP_FILE_n%` | Ignore si plus de n% de lignes invalides | QualitÃ© relative acceptable |

---

### 5.3 VALIDATION_MODE â€” tester avant de charger

```sql
-- Dry run : liste toutes les erreurs sans charger une seule ligne
COPY INTO RAW_EVENTS
FROM @gcs_stage/HealthApp_2k.log
FILE_FORMAT      = (FORMAT_NAME = 'csv_file')
VALIDATION_MODE  = 'RETURN_ALL_ERRORS';   -- â† aucune donnÃ©e chargÃ©e
-- â†’ Retourne : ERROR, FILE, LINE, CHARACTER, BYTE_OFFSET, CATEGORY, CODE, SQL_STATE, COLUMN_NAME, ROW_NUMBER, ROW_START_LINE, REJECTED_RECORD

-- Retourner seulement les N premiÃ¨res erreurs (plus rapide sur gros fichiers)
COPY INTO RAW_EVENTS
FROM @gcs_stage/HealthApp_2k.log
FILE_FORMAT      = (FORMAT_NAME = 'csv_file')
VALIDATION_MODE  = 'RETURN_ERRORS';       -- â† retourne les erreurs du dernier COPY INTO
```

> ğŸ’¡ **Workflow recommandÃ©** : toujours faire un `VALIDATION_MODE = 'RETURN_ALL_ERRORS'` sur un Ã©chantillon avant le premier chargement en production. Permet de corriger le File Format sans toucher les donnÃ©es.

---

### 5.4 COPY INTO avec transformation Ã  la volÃ©e

Snowflake permet de transformer les donnÃ©es pendant le chargement avec un sous-SELECT :

```sql
-- Chargement avec cast de types et transformation
COPY INTO RAW_EVENTS (event_id, event_timestamp, process_name, process_id, message)
FROM (
    SELECT
        $1::INTEGER                        AS event_id,        -- cast en entier
        TO_TIMESTAMP($2, 'MM/DD/YYYY HH24:MI:SS') AS event_timestamp,  -- parse date custom
        UPPER(TRIM($3))                    AS process_name,    -- nettoyage texte
        $4::INTEGER                        AS process_id,
        $5                                 AS message          -- colonne brute
    FROM @gcs_stage/HealthApp_2k.log
)
FILE_FORMAT = (FORMAT_NAME = 'csv_file')
ON_ERROR    = 'CONTINUE';

-- Charger seulement certaines colonnes d'un fichier plus large
COPY INTO clients (id, nom, email)
FROM (
    SELECT $1, $2, $4     -- $3 = colonne ignorÃ©e (tÃ©lÃ©phone non nÃ©cessaire)
    FROM @gcs_stage/clients_export.csv
)
FILE_FORMAT = (FORMAT_NAME = 'csv_file');
```

> ğŸ’¡ **`$1`, `$2`... sont des rÃ©fÃ©rences positionnelles** aux colonnes du fichier brut (indÃ©pendantes des noms de colonnes de la table cible). TrÃ¨s utile quand le fichier a plus de colonnes que la table.

---

### 5.5 VÃ©rifier l'historique de chargement

```sql
-- Historique des chargements sur une table (30 derniers jours max)
SELECT
    FILE_NAME,
    STATUS,
    ROW_COUNT,
    ROW_PARSED,
    ERROR_COUNT,
    FIRST_ERROR,
    FIRST_ERROR_LINE,
    FIRST_ERROR_COLUMN_NAME,
    LAST_LOAD_TIME
FROM TABLE(INFORMATION_SCHEMA.COPY_HISTORY(
    TABLE_NAME => 'RAW_EVENTS',
    START_TIME => DATEADD('HOURS', -24, CURRENT_TIMESTAMP())
))
ORDER BY LAST_LOAD_TIME DESC;

-- Voir uniquement les fichiers avec des erreurs
SELECT FILE_NAME, STATUS, ERROR_COUNT, FIRST_ERROR
FROM TABLE(INFORMATION_SCHEMA.COPY_HISTORY(
    TABLE_NAME => 'RAW_EVENTS',
    START_TIME => DATEADD('HOURS', -24, CURRENT_TIMESTAMP())
))
WHERE STATUS = 'LOAD_FAILED' OR ERROR_COUNT > 0
ORDER BY LAST_LOAD_TIME DESC;

-- Valider les erreurs du dernier COPY INTO (aprÃ¨s exÃ©cution)
SELECT * FROM TABLE(VALIDATE(
    RAW_EVENTS,
    JOB_ID => '_last'
));
```

ğŸ“– [COPY INTO \<table\>](https://docs.snowflake.com/fr/sql-reference/sql/copy-into-table) | [COPY_HISTORY](https://docs.snowflake.com/fr/sql-reference/functions/copy_history)

---

## 6. Workflow Complet GCS â†’ Snowflake

### Objectifs de cette section
- Reproduire le pipeline health_app de bout en bout
- Comprendre le rÃ´le prÃ©cis de chaque composant
- Valider le chargement Ã©tape par Ã©tape

---

### 6.1 Contexte du projet health_app

**ScÃ©nario** : l'application mobile `health_app` gÃ©nÃ¨re des logs applicatifs stockÃ©s dans GCS. L'objectif est de charger le fichier `HealthApp_2k.log` dans la table `RAW_EVENTS` pour analyse.

```
Fichier source : gcs://ada_snowflake/HealthApp_2k.log
Format         : CSV, sÃ©parateur virgule, 1 ligne d'en-tÃªte
Destination    : HEALTH_APP.RAW.RAW_EVENTS
```

---

### 6.2 Ã‰tape 1 â€” DÃ©finir le contexte

```sql
-- SÃ©lectionner le rÃ´le avec les droits nÃ©cessaires
-- ACCOUNTADMIN requis pour CREATE STORAGE INTEGRATION uniquement
USE ROLE ACCOUNTADMIN;
USE DATABASE HEALTH_APP;
USE SCHEMA RAW;
```

> ğŸ’¡ En production, crÃ©er un rÃ´le dÃ©diÃ© `data_loader_role` avec uniquement les droits nÃ©cessaires. `ACCOUNTADMIN` ne doit pas Ãªtre utilisÃ© pour les opÃ©rations courantes. Voir Chapitre RBAC.

---

### 6.3 Ã‰tape 2 â€” Storage Integration GCS

```sql
CREATE OR REPLACE STORAGE INTEGRATION gcs_int_ada
    TYPE                      = EXTERNAL_STAGE
    STORAGE_PROVIDER          = GCS
    ENABLED                   = TRUE
    STORAGE_ALLOWED_LOCATIONS = ('gcs://ada_snowflake/');

-- RÃ©cupÃ©rer l'email de service Snowflake (Ã  donner aux droits GCS)
DESC INTEGRATION gcs_int_ada;
-- â†’ Copier STORAGE_GCP_SERVICE_ACCOUNT
-- Exemple : sn12345@gcpserviceaccount.iam.gserviceaccount.com
```

---

### 6.4 Ã‰tape 3 â€” Configurer les droits GCS

Dans la **Google Cloud Console** :

1. `Cloud Storage` â†’ Votre bucket (`ada_snowflake`) â†’ onglet **Permissions**
2. `+ Grant Access` â†’ coller l'email Snowflake dans **New principals**
3. Role : **Storage Object Viewer** (lecture seule suffit pour COPY INTO)
4. Cliquer **Save**

> âš ï¸ **DÃ©lai IAM** : les changements de permissions GCS peuvent prendre 1-2 minutes Ã  se propager. Si `LIST @gcs_stage` retourne encore une erreur, patienter quelques secondes.

---

### 6.5 Ã‰tape 4 â€” File Format

```sql
-- Format CSV adaptÃ© au fichier HealthApp_2k.log
CREATE OR REPLACE FILE FORMAT csv_file
    TYPE                         = CSV
    FIELD_DELIMITER              = ','
    SKIP_HEADER                  = 1
    FIELD_OPTIONALLY_ENCLOSED_BY = '"'
    NULL_IF                      = ('NULL', 'null', '')
    EMPTY_FIELD_AS_NULL          = TRUE
    TRIM_SPACE                   = TRUE;
```

---

### 6.6 Ã‰tape 5 â€” Stage Externe GCS

```sql
CREATE OR REPLACE STAGE gcs_stage
    URL                 = 'gcs://ada_snowflake/'
    STORAGE_INTEGRATION = gcs_int_ada
    FILE_FORMAT         = csv_file
    COMMENT             = 'Stage GCS principal â€” projet health_app';
```

---

### 6.7 Ã‰tape 6 â€” Tester l'accÃ¨s et inspecter le fichier

```sql
-- VÃ©rifier que Snowflake voit les fichiers du bucket
LIST @gcs_stage;

-- Inspecter les premiÃ¨res lignes du fichier brut (sans charger)
SELECT $1, $2, $3, $4, $5
FROM @gcs_stage/HealthApp_2k.log
    (FILE_FORMAT => (TYPE = 'CSV' SKIP_HEADER = 1))
LIMIT 10;
-- â†’ Permet de vÃ©rifier : nb colonnes, sÃ©parateur, format dates, valeurs nulles
```

---

### 6.8 Ã‰tape 7 â€” Valider avant de charger (Dry Run)

```sql
-- Dry run : liste toutes les erreurs sans insÃ©rer une seule ligne
COPY INTO RAW_EVENTS
FROM @gcs_stage
FILES            = ('HealthApp_2k.log')
FILE_FORMAT      = (FORMAT_NAME = 'csv_file')
VALIDATION_MODE  = 'RETURN_ALL_ERRORS';
-- Si aucune erreur â†’ 0 lignes retournÃ©es â†’ prÃªt pour le chargement rÃ©el
```

---

### 6.9 Ã‰tape 8 â€” Charger les donnÃ©es

```sql
COPY INTO RAW_EVENTS
FROM @gcs_stage
FILES    = ('HealthApp_2k.log')    -- spÃ©cifier le fichier exact
ON_ERROR = 'CONTINUE'              -- continuer si quelques lignes sont invalides
PURGE    = FALSE;                  -- conserver le fichier dans GCS pour audit
```

---

### 6.10 Ã‰tape 9 â€” VÃ©rifier le chargement

```sql
-- Volume total chargÃ©
SELECT COUNT(*) AS total_lignes FROM RAW_EVENTS;

-- VÃ©rification visuelle d'un Ã©chantillon
SELECT * FROM RAW_EVENTS LIMIT 20;

-- RÃ©partition par process_name (sanity check)
SELECT process_name, COUNT(*) AS nb_events
FROM RAW_EVENTS
GROUP BY 1
ORDER BY 2 DESC;

-- Historique du chargement
SELECT FILE_NAME, STATUS, ROW_COUNT, ERROR_COUNT, LAST_LOAD_TIME
FROM TABLE(INFORMATION_SCHEMA.COPY_HISTORY(
    TABLE_NAME => 'RAW_EVENTS',
    START_TIME => DATEADD('HOURS', -1, CURRENT_TIMESTAMP())
));
```

---

### 6.11 RÃ©sumÃ© visuel â€” rÃ´le de chaque composant

```mermaid
graph LR
    subgraph Securite["ğŸ” SÃ©curitÃ©"]
        A["Storage Integration\ngcs_int_ada\nâ†’ Contient les credentials IAM\nSnowflake accÃ¨de Ã  GCS\nsans clÃ© exposÃ©e dans le SQL"]
    end
    subgraph Localisation["ğŸ—ºï¸ Localisation"]
        B["External Stage\ngcs_stage\nâ†’ Pointeur vers\ngcs://ada_snowflake/\nne stocke rien en local"]
    end
    subgraph Format["ğŸ“„ Format"]
        C["File Format\ncsv_file\nâ†’ RÃ¨gles de parsing\nSKIP_HEADER, NULL_IF\nFIELD_DELIMITER..."]
    end
    subgraph Chargement["ğŸ“¥ Chargement"]
        D["COPY INTO\nRAW_EVENTS\nâ†’ Lit le fichier\nparallÃ©lise, parse\net insÃ¨re"]
    end
    subgraph Destination["ğŸ—ƒï¸ Destination"]
        E["Table RAW_EVENTS\nâ†’ DonnÃ©es disponibles\npour les Tasks\net l'analyse"]
    end

    A --> B --> C --> D --> E
```

ğŸ“– Docs officielles :
- [CREATE STORAGE INTEGRATION](https://docs.snowflake.com/fr/sql-reference/sql/create-storage-integration)
- [Configurer GCS pour Snowflake](https://docs.snowflake.com/fr/user-guide/data-load-gcs-config)
- [COPY INTO \<table\>](https://docs.snowflake.com/fr/sql-reference/sql/copy-into-table)

---

## 7. Chargement INSERT (Petits Volumes)

### Objectifs de cette section
- Utiliser INSERT pour des donnÃ©es statiques ou de test
- Comprendre les limites et quand basculer sur COPY INTO

---

### 7.1 Syntaxes INSERT

```sql
-- â‘  INSERT de plusieurs lignes en une fois (le plus efficace pour <100 lignes)
INSERT INTO RAW_EVENTS (event_id, event_timestamp, process_name, process_id, message)
VALUES
    (1, '2024-01-15 10:30:00', 'Step_LSC',         101, '10:30:00.123 D/Step_LSC test'),
    (2, '2024-01-15 10:30:01', 'HiH_ListenerManager', 102, '10:30:01.456 D/HiH test'),
    (3, '2024-01-15 10:30:02', 'Step_ScreenUtil',   103, '10:30:02.789 D/Screen test');

-- â‘¡ INSERT depuis un SELECT (ETL interne â€” de table Ã  table)
INSERT INTO RAW_EVENTS_ARCHIVE
SELECT *
FROM RAW_EVENTS
WHERE event_timestamp < DATEADD('YEAR', -1, CURRENT_DATE());

-- â‘¢ INSERT avec transformation
INSERT INTO staging.step_lsc (event_timestamp, process_id, message)
SELECT
    event_timestamp,
    process_id,
    TRIM(message)   -- nettoyage Ã  la volÃ©e
FROM RAW_EVENTS
WHERE process_name = 'Step_LSC';
```

---

### 7.2 INSERT vs COPY INTO â€” quand choisir

```mermaid
flowchart TD
    Q1{"Volume de donnÃ©es ?"}
    Q1 -->|"< 100 lignes"| INSERT["âœ… INSERT\nSimple, immÃ©diat\nPas besoin de stage"]
    Q1 -->|"100 â†’ 10 000 lignes"| Q2{"FrÃ©quence ?"}
    Q1 -->|"> 10 000 lignes"| COPY["âœ… COPY INTO\nParallÃ©lisÃ©, optimisÃ©\nDÃ©duplication automatique"]

    Q2 -->|"Ponctuel"| INSERT2["âœ… INSERT ou COPY INTO\nselon la source"]
    Q2 -->|"RÃ©current"| COPY2["âœ… COPY INTO\n+ Task planifiÃ©e"]

    style INSERT fill:#00b894,color:#fff
    style INSERT2 fill:#74b9ff,color:#333
    style COPY fill:#0984e3,color:#fff
    style COPY2 fill:#0984e3,color:#fff
```

> âš ï¸ `INSERT` crÃ©e une micro-transaction par exÃ©cution. Pour des millions de lignes, toujours prÃ©fÃ©rer `COPY INTO` qui est massivement parallÃ©lisÃ© et 10-100x plus rapide.

ğŸ“– [INSERT](https://docs.snowflake.com/fr/sql-reference/sql/insert)

---

## 8. Snowpipe : Ingestion Continue

### Objectifs de cette section
- Comprendre l'architecture Snowpipe et son modÃ¨le de coÃ»t
- CrÃ©er un pipe avec `AUTO_INGEST`
- Monitorer et administrer un pipe en production

---

### 8.1 Comment fonctionne Snowpipe

```mermaid
sequenceDiagram
    participant App as ğŸ–¥ï¸ Application
    participant GCS as â˜ï¸ GCS Bucket
    participant Notif as ğŸ“¢ GCS Pub/Sub
    participant Pipe as ğŸ”§ Snowpipe\n(serverless)
    participant Table as ğŸ—ƒï¸ Table Snowflake

    App->>GCS: â‘  DÃ©pose un nouveau fichier\nevents_2024_01_15_10h.json
    GCS->>Notif: â‘¡ Notification "object created"\n(GCS Event)
    Notif->>Pipe: â‘¢ DÃ©clenche AUTO_INGEST\n(via notification_channel)
    Pipe->>GCS: â‘£ Lit le nouveau fichier
    Pipe->>Table: â‘¤ COPY INTO (micro-batch\nsnowflake serverless)
    Table-->>App: â‘¥ DonnÃ©es disponibles\nen quelques secondes
```

---

### 8.2 COPY INTO vs Snowpipe â€” comparaison dÃ©taillÃ©e

| | COPY INTO (Batch) | Snowpipe (Near-realtime) |
|---|---|---|
| DÃ©clenchement | Manuel ou Task planifiÃ©e | Automatique dÃ¨s qu'un fichier arrive |
| Latence | Minutes (selon schedule) | Secondes |
| Compute | Virtual Warehouse (WH classique) | Serverless (gÃ©rÃ© Snowflake) |
| Facturation | Temps de WH actif | Ã€ la seconde de traitement |
| Cas optimal | Gros fichiers, batch nocturne | Petits fichiers frÃ©quents |
| DÃ©duplication | Oui (metadata COPY INTO) | Oui (metadata Snowpipe) |
| Monitoring | `COPY_HISTORY` | `COPY_HISTORY` + `SYSTEM$PIPE_STATUS` |

```mermaid
quadrantChart
    title Choisir entre COPY INTO et Snowpipe
    x-axis Basse frequence ... Haute frequence
    y-axis Petit volume ... Grand volume
    quadrant-1 Snowpipe ideal
    quadrant-2 Snowpipe ou Streaming API
    quadrant-3 INSERT ou COPY INTO ponctuel
    quadrant-4 COPY INTO batch planifie
    COPY INTO batch: [0.2, 0.8]
    Snowpipe: [0.75, 0.55]
    INSERT: [0.1, 0.12]
    Streaming API: [0.92, 0.85]
    COPY INTO ponctuel: [0.15, 0.35]
```

---

### 8.3 CrÃ©er un Pipe â€” syntaxe annotÃ©e

```sql
-- â‘  CrÃ©er le format de fichier (JSON pour les events streamÃ©s)
CREATE OR REPLACE FILE FORMAT fmt_events_json
    TYPE              = JSON
    STRIP_OUTER_ARRAY = TRUE;   -- [{...},{...}] â†’ lignes sÃ©parÃ©es

-- â‘¡ CrÃ©er le stage externe (voir Section 3)
-- CREATE OR REPLACE STAGE gcs_stage ...

-- â‘¢ CrÃ©er le pipe
CREATE OR REPLACE PIPE pipe_raw_events
    AUTO_INGEST = TRUE   -- dÃ©clenchement automatique via GCS Pub/Sub
    COMMENT     = 'Ingestion continue des events health_app depuis GCS'
AS
-- Le COPY INTO est le corps du pipe (dÃ©finition statique)
COPY INTO RAW_EVENTS
FROM (
    SELECT $1        -- tout le JSON dans une colonne VARIANT
    FROM @gcs_stage/events/
)
FILE_FORMAT = (FORMAT_NAME = 'fmt_events_json')
ON_ERROR    = 'CONTINUE';

-- â‘£ RÃ©cupÃ©rer le Notification Channel (SQS pour S3 / Pub/Sub pour GCS)
-- â†’ Copier notification_channel pour configurer GCS
DESC PIPE pipe_raw_events;
-- Colonne notification_channel : projects/PROJECT_ID/subscriptions/SF_SUB_...
```

---

### 8.4 Configurer GCS Pub/Sub pour AUTO_INGEST

AprÃ¨s `DESC PIPE` :

1. Dans **GCS Console** : `Pub/Sub` â†’ `Topics` â†’ crÃ©er un topic pour le bucket
2. `Cloud Storage` â†’ bucket â†’ `Notifications` â†’ configurer sur le topic
3. `Pub/Sub` â†’ crÃ©er une **Subscription** push vers le `notification_channel` de Snowflake

> ğŸ“– [Configurer Snowpipe avec GCS](https://docs.snowflake.com/fr/user-guide/data-load-snowpipe-auto-gcs)

---

### 8.5 Administrer et monitorer Snowpipe

```sql
-- Statut complet du pipe (JSON enrichi)
SELECT PARSE_JSON(SYSTEM$PIPE_STATUS('pipe_raw_events'));
-- â†’ pendingFileCount, notificationChannelName, lastIngestedTimestamp...

-- Historique des ingestions (derniÃ¨re heure)
SELECT
    FILE_NAME,
    STATUS,
    ROW_COUNT,
    ERROR_COUNT,
    FIRST_ERROR,
    LAST_LOAD_TIME
FROM TABLE(INFORMATION_SCHEMA.COPY_HISTORY(
    TABLE_NAME => 'RAW_EVENTS',
    START_TIME => DATEADD('HOURS', -1, CURRENT_TIMESTAMP())
))
ORDER BY LAST_LOAD_TIME DESC;

-- Pause / Reprise (maintenance, incident)
ALTER PIPE pipe_raw_events PAUSE;
ALTER PIPE pipe_raw_events RESUME;

-- Forcer le re-chargement de fichiers spÃ©cifiques
ALTER PIPE pipe_raw_events REFRESH
    PREFIX     = 'events/2024-01-15/'   -- dossier spÃ©cifique
    MODIFIED_AFTER = '2024-01-15T00:00:00Z';

-- Voir tous les pipes du schÃ©ma
SHOW PIPES IN SCHEMA HEALTH_APP.RAW;
```

ğŸ“– [Snowpipe Introduction](https://docs.snowflake.com/fr/user-guide/data-load-snowpipe-intro) | [Snowpipe Streaming](https://docs.snowflake.com/fr/user-guide/snowpipe-streaming/data-load-snowpipe-streaming-overview)

---

## 9. PUT & GET : CLI SnowSQL

### Objectifs de cette section
- Installer SnowSQL et uploader des fichiers locaux vers un stage interne
- Utiliser le connecteur Python pour automatiser PUT + COPY INTO
- Comprendre les contraintes de PUT/GET

---

### 9.1 Contraintes importantes

```mermaid
graph TD
    A["PUT / GET"] --> B{"Disponible dans ?"}
    B --> C["âœ… SnowSQL CLI\nsnowsql -a account -u user"]
    B --> D["âœ… Python Connector\nsnowflake-connector-python"]
    B --> E["âœ… JDBC/ODBC Driver"]
    B --> F["âŒ Snowsight (UI web)\nNon supportÃ©"]
    B --> G["âŒ Snowpark\nNon supportÃ©"]

    style C fill:#00b894,color:#fff
    style D fill:#00b894,color:#fff
    style E fill:#00b894,color:#fff
    style F fill:#e17055,color:#fff
    style G fill:#e17055,color:#fff
```

---

### 9.2 Installer SnowSQL

```bash
# â”€â”€ macOS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
brew install --cask snowflake-snowsql

# â”€â”€ Linux (x86_64) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
curl -O https://sfc-repo.snowflakecomputing.com/snowsql/bootstrap/1.2/linux_x86_64/snowsql-1.2.9-linux_x86_64.bash
bash snowsql-1.2.9-linux_x86_64.bash

# â”€â”€ Connexion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
snowsql -a <account_identifier> -u <username>
# account_identifier : ex: abc12345.eu-west-1 (visible dans l'URL Snowsight)
```

---

### 9.3 PUT â€” uploader vers un Stage Interne

```sql
-- Uploader un fichier local vers le stage user (@~)
PUT file:///chemin/absolu/data.csv @~;

-- Uploader avec options complÃ¨tes annotÃ©es
PUT file:///data/HealthApp_2k.log @mon_stage
    AUTO_COMPRESS = TRUE    -- â‘  compresser en .gz avant upload (rÃ©duit la taille)
    PARALLEL      = 4       -- â‘¡ threads parallÃ¨les (dÃ©faut 4, max 99)
    OVERWRITE     = FALSE;  -- â‘¢ FALSE = ne pas Ã©craser si fichier dÃ©jÃ  prÃ©sent

-- Uploader plusieurs fichiers avec wildcard
PUT file:///data/logs/*.log @mon_stage AUTO_COMPRESS=TRUE;

-- VÃ©rifier que le fichier est bien arrivÃ©
LIST @mon_stage;
```

---

### 9.4 GET â€” tÃ©lÃ©charger depuis un Stage

```sql
-- TÃ©lÃ©charger un fichier spÃ©cifique
GET @mon_stage/data.csv.gz file:///tmp/output/;

-- TÃ©lÃ©charger tous les fichiers du stage
GET @mon_stage file:///tmp/output/ PARALLEL = 8;
```

---

### 9.5 Pipeline complet via Python Connector

```python
import snowflake.connector
import os

# â‘  Connexion Snowflake
conn = snowflake.connector.connect(
    account   = os.environ['SNOWFLAKE_ACCOUNT'],   # ne jamais hardcoder les credentials
    user      = os.environ['SNOWFLAKE_USER'],
    password  = os.environ['SNOWFLAKE_PASSWORD'],
    database  = 'HEALTH_APP',
    schema    = 'RAW',
    warehouse = 'COMPUTE_WH'
)

cur = conn.cursor()

try:
    # â‘¡ Upload du fichier local vers le stage interne
    cur.execute("""
        PUT file:///tmp/HealthApp_2k.log @mon_stage
        AUTO_COMPRESS = TRUE
        OVERWRITE     = FALSE
    """)
    print("PUT terminÃ© :", cur.fetchall())

    # â‘¢ Chargement dans la table
    cur.execute("""
        COPY INTO RAW_EVENTS
        FROM @mon_stage/HealthApp_2k.log.gz
        FILE_FORMAT = (FORMAT_NAME = 'csv_file')
        ON_ERROR    = 'CONTINUE'
    """)
    result = cur.fetchall()
    print("COPY INTO terminÃ© :", result)

    # â‘£ VÃ©rification
    cur.execute("SELECT COUNT(*) FROM RAW_EVENTS")
    count = cur.fetchone()[0]
    print(f"Lignes chargÃ©es : {count}")

    # â‘¤ Nettoyage du stage
    cur.execute("REMOVE @mon_stage/HealthApp_2k.log.gz")

except Exception as e:
    print(f"Erreur : {e}")
    raise
finally:
    cur.close()
    conn.close()
```

ğŸ“– [PUT](https://docs.snowflake.com/fr/sql-reference/sql/put) | [GET](https://docs.snowflake.com/fr/sql-reference/sql/get) | [Python Connector](https://docs.snowflake.com/fr/user-guide/python-connector-example)

---

## 10. Formats de Fichiers SupportÃ©s

### 10.1 Vue d'ensemble

```mermaid
mindmap
  root((File Formats\nSnowflake))
    StructurÃ©s plats
      CSV
        FIELD_DELIMITER
        SKIP_HEADER
        NULL_IF
      TSV
        Tab-separated
    Semi-structurÃ©s
      JSON
        VARIANT
        STRIP_OUTER_ARRAY
        Notation colon
      AVRO
        Kafka / Streaming
        Schema intÃ©grÃ©
      XML
        Legacy ERP
        XMLGET
    Columnar Analytics
      Parquet
        Snappy compression
        MATCH_BY_COLUMN_NAME
        Data Lake
      ORC
        Hadoop Hive
        TrÃ¨s compact
```

---

### 10.2 Tableau comparatif complet

| Format | Extension | Compression | Semi-structurÃ© | Perf. lecture | RecommandÃ© pour |
|---|---|---|---|---|---|
| CSV | `.csv` | Optionnelle (`.gz`) | Non | Moyenne | Exports Excel, fichiers plats |
| TSV | `.tsv` | Optionnelle | Non | Moyenne | Exports avec virgules dans les valeurs |
| JSON | `.json` | Optionnelle | âœ… Oui | Moyenne | APIs REST, logs applicatifs, events |
| Parquet | `.parquet` | âœ… Auto (Snappy) | Partiel | âš¡ Excellente | Data Lake, Analytics, migrations |
| ORC | `.orc` | âœ… Auto | Non | âš¡ Excellente | Ã‰cosystÃ¨me Hadoop/Hive |
| AVRO | `.avro` | Optionnelle | âœ… Oui | Bonne | Kafka, streaming, schÃ©ma Ã©volutif |
| XML | `.xml` | Non | âœ… Oui | Faible | Legacy ERP, SOAP, fichiers config |

---

### 10.3 RequÃªter les donnÃ©es semi-structurÃ©es (JSON/AVRO)

```sql
-- AprÃ¨s COPY INTO dans une colonne VARIANT :
CREATE OR REPLACE TABLE raw_events_json (data VARIANT);

COPY INTO raw_events_json
FROM @gcs_stage/events.json
FILE_FORMAT = (FORMAT_NAME = 'fmt_json');

-- AccÃ¨s aux champs JSON
SELECT
    data:event_id::INTEGER          AS event_id,
    data:process_name::VARCHAR      AS process_name,
    data:timestamp::TIMESTAMP       AS ts,
    data:properties:cpu_usage       AS cpu_usage,          -- champ imbriquÃ©
    data:tags[0]::VARCHAR           AS first_tag,          -- premier Ã©lÃ©ment d'un tableau
    FLATTEN(data:tags)              AS tag                 -- dÃ©rouler un tableau
FROM raw_events_json;

-- Aplatir une colonne JSON vers une vraie table relationnelle
CREATE OR REPLACE VIEW events_flat AS
SELECT
    data:event_id::INTEGER        AS event_id,
    data:process_name::VARCHAR    AS process_name,
    data:process_id::INTEGER      AS process_id,
    data:timestamp::TIMESTAMP     AS event_timestamp,
    data:message::VARCHAR         AS message
FROM raw_events_json;
```

ğŸ“– [Semi-structured data](https://docs.snowflake.com/fr/user-guide/semistructured-intro) | [VARIANT type](https://docs.snowflake.com/fr/sql-reference/data-types-semistructured)

---

## 11. Cas Pratiques Complets

### ğŸ”µ Cas 1 â€” Premier chargement du fichier `HealthApp_2k.log`

**ScÃ©nario** : environnement de staging, premier chargement depuis zÃ©ro.

```sql
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- SETUP COMPLET â€” ordre Ã  respecter
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- â‘  Contexte
USE ROLE ACCOUNTADMIN;
USE DATABASE HEALTH_APP;
USE SCHEMA RAW;

-- â‘¡ Table de destination
CREATE OR REPLACE TABLE RAW_EVENTS (
    event_id        INTEGER,
    event_timestamp TIMESTAMP,
    process_name    VARCHAR(100),
    process_id      INTEGER,
    message         VARCHAR(16777216)
);

-- â‘¢ File Format
CREATE OR REPLACE FILE FORMAT csv_file
    TYPE                         = CSV
    FIELD_DELIMITER              = ','
    SKIP_HEADER                  = 1
    FIELD_OPTIONALLY_ENCLOSED_BY = '"'
    NULL_IF                      = ('NULL', 'null', '')
    EMPTY_FIELD_AS_NULL          = TRUE
    TRIM_SPACE                   = TRUE;

-- â‘£ Storage Integration GCS
CREATE OR REPLACE STORAGE INTEGRATION gcs_int_ada
    TYPE                      = EXTERNAL_STAGE
    STORAGE_PROVIDER          = GCS
    ENABLED                   = TRUE
    STORAGE_ALLOWED_LOCATIONS = ('gcs://ada_snowflake/');

-- â†’ DESC INTEGRATION gcs_int_ada â†’ rÃ©cupÃ©rer STORAGE_GCP_SERVICE_ACCOUNT
-- â†’ Configurer les droits dans GCS Console (Storage Object Viewer)

-- â‘¤ Stage externe
CREATE OR REPLACE STAGE gcs_stage
    URL                 = 'gcs://ada_snowflake/'
    STORAGE_INTEGRATION = gcs_int_ada
    FILE_FORMAT         = csv_file;

-- â‘¥ Test accÃ¨s
LIST @gcs_stage;

-- â‘¦ Inspecter le fichier brut
SELECT $1, $2, $3, $4, $5
FROM @gcs_stage/HealthApp_2k.log
    (FILE_FORMAT => (TYPE = 'CSV' SKIP_HEADER = 1))
LIMIT 5;

-- â‘§ Dry run
COPY INTO RAW_EVENTS
FROM @gcs_stage
FILES           = ('HealthApp_2k.log')
FILE_FORMAT     = (FORMAT_NAME = 'csv_file')
VALIDATION_MODE = 'RETURN_ALL_ERRORS';

-- â‘¨ Chargement rÃ©el
COPY INTO RAW_EVENTS
FROM @gcs_stage
FILES    = ('HealthApp_2k.log')
ON_ERROR = 'CONTINUE'
PURGE    = FALSE;

-- â‘© VÃ©rification
SELECT COUNT(*) AS total FROM RAW_EVENTS;
SELECT process_name, COUNT(*) FROM RAW_EVENTS GROUP BY 1 ORDER BY 2 DESC;
SELECT FILE_NAME, STATUS, ROW_COUNT, ERROR_COUNT
FROM TABLE(INFORMATION_SCHEMA.COPY_HISTORY(
    TABLE_NAME => 'RAW_EVENTS',
    START_TIME => DATEADD('MINUTES', -10, CURRENT_TIMESTAMP())
));
```

---

### ğŸŸ¡ Cas 2 â€” Chargement rÃ©current avec gestion d'erreurs et audit

**ScÃ©nario** : les fichiers `HealthApp_*.log` arrivent chaque heure dans GCS. Un rapport d'audit doit Ãªtre produit aprÃ¨s chaque chargement.

```sql
-- Table d'audit des chargements (traÃ§abilitÃ© complÃ¨te)
CREATE OR REPLACE TABLE RAW.loading_audit (
    loaded_at       TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
    file_name       VARCHAR(500),
    status          VARCHAR(50),   -- 'SUCCEEDED', 'PARTIAL', 'FAILED'
    rows_loaded     INTEGER,
    rows_error      INTEGER,
    first_error     VARCHAR(2000),
    notes           VARCHAR(1000)
);

-- ProcÃ©dure de chargement avec audit automatique
CREATE OR REPLACE PROCEDURE RAW.load_health_events(pattern STRING)
RETURNS STRING
LANGUAGE SQL
EXECUTE AS CALLER
AS $$
DECLARE
    load_exception EXCEPTION (-9999, 'Erreur lors du chargement des events');
BEGIN
    -- â‘  Chargement avec pattern dynamique (ex: '.*2024-01-15.*\.log')
    COPY INTO RAW_EVENTS
    FROM @gcs_stage
    PATTERN  = :pattern
    ON_ERROR = 'CONTINUE'
    PURGE    = FALSE;

    -- â‘¡ Audit automatique depuis COPY_HISTORY
    INSERT INTO RAW.loading_audit (file_name, status, rows_loaded, rows_error, first_error)
    SELECT
        FILE_NAME,
        CASE
            WHEN ERROR_COUNT = 0         THEN 'SUCCEEDED'
            WHEN ROW_COUNT > 0           THEN 'PARTIAL'
            ELSE                              'FAILED'
        END AS status,
        ROW_COUNT,
        ERROR_COUNT,
        FIRST_ERROR
    FROM TABLE(INFORMATION_SCHEMA.COPY_HISTORY(
        TABLE_NAME => 'RAW_EVENTS',
        START_TIME => DATEADD('MINUTES', -5, CURRENT_TIMESTAMP())
    ));

    RETURN 'Chargement terminÃ© â€” voir RAW.loading_audit';

EXCEPTION
    WHEN OTHER THEN
        INSERT INTO RAW.loading_audit (file_name, status, notes)
        VALUES (:pattern, 'FAILED', :SQLERRM);
        RAISE load_exception;
END;
$$;

-- Appel de la procÃ©dure (manuellement ou via Task)
CALL RAW.load_health_events('.*HealthApp.*\.log');

-- Consulter l'audit
SELECT * FROM RAW.loading_audit ORDER BY loaded_at DESC LIMIT 20;

-- KPI : taux de succÃ¨s sur 7 jours
SELECT
    DATE_TRUNC('day', loaded_at)   AS jour,
    COUNT(*)                       AS total_fichiers,
    SUM(CASE WHEN status = 'SUCCEEDED' THEN 1 ELSE 0 END) AS succes,
    SUM(CASE WHEN status = 'FAILED'    THEN 1 ELSE 0 END) AS echecs,
    SUM(rows_loaded)               AS total_lignes_chargees,
    SUM(rows_error)                AS total_lignes_rejetees
FROM RAW.loading_audit
WHERE loaded_at >= DATEADD('day', -7, CURRENT_DATE())
GROUP BY 1
ORDER BY 1 DESC;
```

---

### ğŸŸ¢ Cas 3 â€” Migration depuis un Data Lake Parquet vers Snowflake

**ScÃ©nario** : migration d'un historique de 3 ans de logs Parquet stockÃ©s dans GCS vers `staging.step_lsc`.

```sql
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- MIGRATION DATA LAKE PARQUET â†’ SNOWFLAKE
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- â‘  Stage dÃ©diÃ© au Data Lake Parquet
CREATE OR REPLACE STAGE stage_datalake_parquet
    URL                 = 'gcs://ada_snowflake/datalake/step_lsc/'
    STORAGE_INTEGRATION = gcs_int_ada
    COMMENT             = 'Data Lake historique â€” fichiers Parquet 2021-2024';

-- â‘¡ Inspecter la structure du fichier Parquet
SELECT $1
FROM @stage_datalake_parquet/step_lsc_2024_01.parquet
    (FILE_FORMAT => (TYPE = 'PARQUET'))
LIMIT 3;
-- â†’ Retourne des lignes VARIANT avec les champs du Parquet
-- â†’ Exemple : {"event_timestamp":"...","process_id":101,"log_trigger":"...","message":"..."}

-- â‘¢ Table de destination
CREATE OR REPLACE TABLE staging.step_lsc (
    event_timestamp TIMESTAMP,
    process_id      INTEGER,
    log_trigger     VARCHAR(200),
    message         VARCHAR(16777216)
);

-- â‘£ Chargement Parquet avec MATCH_BY_COLUMN_NAME
-- (mappe les colonnes par nom, pas par position â€” robuste aux rÃ©organisations)
COPY INTO staging.step_lsc
FROM @stage_datalake_parquet
FILE_FORMAT          = (TYPE = 'PARQUET')
MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE   -- case-insensitive : event_timestamp = EVENT_TIMESTAMP
PATTERN              = '.*step_lsc.*\.parquet'
ON_ERROR             = 'CONTINUE';

-- â‘¤ VÃ©rification post-migration
SELECT COUNT(*)                              AS total_lignes  FROM staging.step_lsc;
SELECT MIN(event_timestamp), MAX(event_timestamp) AS plage_dates FROM staging.step_lsc;

-- â‘¥ ContrÃ´le qualitÃ© : lignes avec message vide (suspect)
SELECT COUNT(*) AS messages_vides
FROM staging.step_lsc
WHERE message IS NULL OR TRIM(message) = '';

-- â‘¦ VÃ©rification via COPY_HISTORY
SELECT FILE_NAME, STATUS, ROW_COUNT, ERROR_COUNT
FROM TABLE(INFORMATION_SCHEMA.COPY_HISTORY(
    TABLE_NAME => 'STEP_LSC',
    START_TIME => DATEADD('HOURS', -2, CURRENT_TIMESTAMP())
))
ORDER BY LAST_LOAD_TIME DESC;
```

---

## 12. ğŸ”´ Dictionnaire des Erreurs Communes

### 12.1 Arbre de dÃ©cision â€” diagnostiquer une erreur de chargement

```mermaid
flowchart TD
    START(["âŒ Erreur lors du chargement"]) --> Q1{OÃ¹ se situe l'erreur ?}

    Q1 -->|"Avant COPY INTO\n(Stage / Integration)"| B1["Chemin A â€” AccÃ¨s"]
    Q1 -->|"Pendant COPY INTO\n(parsing fichier)"| B2["Chemin B â€” Format"]
    Q1 -->|"COPY INTO rÃ©ussi\nmais donnÃ©es manquantes"| B3["Chemin C â€” DÃ©duplication"]
    Q1 -->|"Snowpipe\nne charge pas"| B4["Chemin D â€” Snowpipe"]
    Q1 -->|"Erreur permissions"| B5["Chemin E â€” Droits"]

    B1 --> C1{"LIST @stage\nfonctionne ?"}
    C1 -->|"Non â€” Access Denied"| F1["VÃ©rifier Storage Integration\nDESC INTEGRATION\nConfigurer IAM/GCS permissions"]
    C1 -->|"Non â€” Stage not found"| F2["SHOW STAGES;\nVÃ©rifier nom et schÃ©ma\nGRANT USAGE ON STAGE ..."]
    C1 -->|"Oui mais fichier absent"| F3["VÃ©rifier le chemin\nLes chemins sont CASE-SENSITIVE\nLIST @stage/dossier/"]

    B2 --> C2{"Type d'erreur\ndans COPY_HISTORY ?"}
    C2 -->|"Number of columns mismatch"| F4["Inspecter le fichier brut\nSELECT $1,$2 FROM @stage LIMIT 5\nVÃ©rifier FIELD_DELIMITER"]
    C2 -->|"Date not recognized"| F5["SpÃ©cifier DATE_FORMAT\nou utiliser TRY_TO_DATE()\nVÃ©rifier le format rÃ©el du fichier"]
    C2 -->|"Numeric value not recognized"| F6["Ajouter NULL_IF = ('')\nEMPTY_FIELD_AS_NULL = TRUE"]
    C2 -->|"Max row size exceeded"| F7["STRIP_OUTER_ARRAY = TRUE si JSON\nSplitter le fichier si CSV"]
    C2 -->|"File compressed but no compression"| F8["COMPRESSION = 'AUTO'\ndans le File Format"]

    B3 --> C3{"COPY_HISTORY :\nstatus du fichier ?"}
    C3 -->|"LOAD_IN_PROGRESS ou LOADED"| F9["Fichier dÃ©jÃ  chargÃ© â€” dÃ©duplication active\nUtiliser FORCE=TRUE si rechargement voulu\nâš ï¸ risque de doublons"]
    C3 -->|"Pas de trace dans COPY_HISTORY"| F10["COPY INTO non exÃ©cutÃ©\nou mauvais nom de fichier (PATTERN/FILES)"]

    B4 --> C4{"SYSTEM$PIPE_STATUS\nindique ?"}
    C4 -->|"PAUSED"| F11["ALTER PIPE mon_pipe RESUME;\nVÃ©rifier la cause de la pause"]
    C4 -->|"pendingFileCount Ã©levÃ©"| F12["Notification Pub/Sub mal configurÃ©e\nVÃ©rifier notification_channel dans DESC PIPE"]
    C4 -->|"stale"| F13["Pipe sans activitÃ© rÃ©cente\nVÃ©rifier que des fichiers arrivent\ndans le bucket"]

    B5 --> F14["SHOW GRANTS TO ROLE mon_role;\nGRANT USAGE ON STAGE\nGRANT INSERT ON TABLE\nGRANT EXECUTE ON WAREHOUSE"]

    style START fill:#e17055,color:#fff
    style F1 fill:#55efc4,color:#333
    style F2 fill:#55efc4,color:#333
    style F4 fill:#55efc4,color:#333
    style F5 fill:#55efc4,color:#333
    style F9 fill:#fdcb6e,color:#333
    style F12 fill:#55efc4,color:#333
    style F14 fill:#55efc4,color:#333
```

---

### 12.2 Erreurs COPY INTO â€” Fichiers & Formats

| Code / Message | Cause | Solution | Doc |
|---|---|---|---|
| `Number of columns in file (N) does not match that of the corresponding table (M)` | Nb colonnes fichier â‰  table | Inspecter avec `SELECT $1,$2 FROM @stage LIMIT 5` â€” vÃ©rifier `FIELD_DELIMITER` | [Troubleshooting](https://docs.snowflake.com/fr/user-guide/data-load-troubleshooting) |
| `Field delimiter ',' found after quoted field` | Guillemets non fermÃ©s dans le CSV | VÃ©rifier `FIELD_OPTIONALLY_ENCLOSED_BY` â€” nettoyer les guillemets mal Ã©chappÃ©s | [File Format](https://docs.snowflake.com/fr/sql-reference/sql/create-file-format) |
| `Numeric value '' is not recognized` | Champ vide lÃ  oÃ¹ un nombre est attendu | Ajouter `NULL_IF = ('')` ou `EMPTY_FIELD_AS_NULL = TRUE` | [File Format](https://docs.snowflake.com/fr/sql-reference/sql/create-file-format) |
| `Date '13/25/2024' is not recognized` | Format de date non reconnu | SpÃ©cifier `DATE_FORMAT = 'DD/MM/YYYY'` ou utiliser `TRY_TO_DATE($col, 'DD/MM/YYYY')` | [TRY_TO_DATE](https://docs.snowflake.com/fr/sql-reference/functions/try_to_date) |
| `Max row size (16777216) exceeded` | Ligne dÃ©passe 16 MB (JSON imbriquÃ© volumineux) | Splitter le fichier, `STRIP_OUTER_ARRAY=TRUE`, prÃ©parer en amont | [Data Prep](https://docs.snowflake.com/fr/user-guide/data-load-considerations-prepare) |
| `File is compressed (gzip) but COPY option FILE_FORMAT specifies NONE` | Fichier `.gz` mais `COMPRESSION = NONE` | Changer `COMPRESSION = 'AUTO'` dans le File Format | [File Format](https://docs.snowflake.com/fr/sql-reference/sql/create-file-format) |
| `Character not UTF-8 encoded` | Fichier encodÃ© en ISO-8859-1 ou Latin-1 | Ajouter `ENCODING = 'ISO-8859-1'` ou convertir le fichier en UTF-8 | [File Format](https://docs.snowflake.com/fr/sql-reference/sql/create-file-format) |
| `COPY INTO already loaded file` | Fichier dÃ©jÃ  chargÃ© â€” dÃ©duplication active | Normal. Utiliser `FORCE = TRUE` pour recharger (âš ï¸ doublons possibles) | [COPY INTO](https://docs.snowflake.com/fr/sql-reference/sql/copy-into-table) |

---

### 12.3 Erreurs d'AccÃ¨s & Stage

| Code / Message | Cause | Solution | Doc |
|---|---|---|---|
| `Stage does not exist or not authorized` | Stage inexistant ou rÃ´le sans droits USAGE | `SHOW STAGES;` â†’ `GRANT USAGE ON STAGE s TO ROLE r;` | [Access Control](https://docs.snowflake.com/fr/user-guide/security-access-control-privileges) |
| `GCS: 403 Access Denied` | Email Snowflake absent des permissions GCS | `DESC INTEGRATION` â†’ ajouter l'email dans GCS > Permissions du bucket | [GCS Config](https://docs.snowflake.com/fr/user-guide/data-load-gcs-config) |
| `Invalid credentials for GCS bucket` | Storage Integration dÃ©sactivÃ©e ou mal crÃ©Ã©e | VÃ©rifier `DESC INTEGRATION` â†’ `ENABLED = TRUE` â€” recrÃ©er si nÃ©cessaire | [Storage Integration](https://docs.snowflake.com/fr/sql-reference/sql/create-storage-integration) |
| `File 'HealthApp_2k.log' not found in stage` | Chemin ou nom incorrect (case-sensitive) | `LIST @gcs_stage;` â†’ les chemins sont sensibles Ã  la casse | [LIST](https://docs.snowflake.com/fr/sql-reference/sql/list) |
| `Location not allowed by integration` | URL du stage hors des `STORAGE_ALLOWED_LOCATIONS` | Modifier la Storage Integration ou ajuster l'URL du stage | [Storage Integration](https://docs.snowflake.com/fr/sql-reference/sql/create-storage-integration) |

---

### 12.4 Erreurs de Permissions

| Code / Message | Cause | Solution | Doc |
|---|---|---|---|
| `Insufficient privileges to operate on table` | RÃ´le sans droit `INSERT` sur la table | `GRANT INSERT ON TABLE RAW_EVENTS TO ROLE mon_role;` | [Privileges](https://docs.snowflake.com/fr/user-guide/security-access-control-privileges) |
| `No active warehouse selected` | `USE WAREHOUSE` non exÃ©cutÃ© ou WH inexistant | `USE WAREHOUSE COMPUTE_WH;` â†’ `SHOW WAREHOUSES;` | [Warehouses](https://docs.snowflake.com/fr/user-guide/warehouses-overview) |
| `Schema does not exist or not authorized` | SchÃ©ma absent ou contexte incorrect | `USE DATABASE HEALTH_APP; USE SCHEMA RAW;` | [USE SCHEMA](https://docs.snowflake.com/fr/sql-reference/sql/use-schema) |
| `Create stage requires a database and a schema` | Pas de contexte DB/Schema dÃ©fini | `USE DATABASE HEALTH_APP; USE SCHEMA RAW;` avant `CREATE STAGE` | [CREATE STAGE](https://docs.snowflake.com/fr/sql-reference/sql/create-stage) |

---

### 12.5 Erreurs Snowpipe SpÃ©cifiques

| Code / Message | Cause | Solution | Doc |
|---|---|---|---|
| `Auto-ingest not triggering` | Notification GCS Pub/Sub non configurÃ©e ou mauvais SQS ARN | `DESC PIPE mon_pipe` â†’ copier `notification_channel` â†’ reconfigurer Pub/Sub | [Snowpipe GCS](https://docs.snowflake.com/fr/user-guide/data-load-snowpipe-auto-gcs) |
| `Files already loaded (skipped by Snowpipe)` | Snowpipe ne recharge pas les fichiers dÃ©jÃ  traitÃ©s | Renommer le fichier ou utiliser `ALTER PIPE ... REFRESH` | [Snowpipe Manage](https://docs.snowflake.com/fr/user-guide/data-load-snowpipe-manage) |
| `Pipe is in PAUSED state` | Pipe mis en pause manuellement ou suite Ã  erreur rÃ©pÃ©tÃ©e | `ALTER PIPE mon_pipe RESUME;` + diagnostiquer la cause | [Snowpipe Manage](https://docs.snowflake.com/fr/user-guide/data-load-snowpipe-manage) |
| `pendingFileCount trÃ¨s Ã©levÃ© dans PIPE_STATUS` | Backlog de fichiers â€” Snowpipe en retard | VÃ©rifier les erreurs dans `COPY_HISTORY` â€” scaling du WH si besoin | [PIPE_STATUS](https://docs.snowflake.com/fr/sql-reference/functions/system_pipe_status) |

---

### 12.6 Commandes de diagnostic rapide

```sql
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- DIAGNOSTIC COMPLET â€” exÃ©cuter dans l'ordre
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- â‘  VÃ©rifier les fichiers visibles dans le stage
LIST @gcs_stage;
LIST @gcs_stage/dossier/;   -- sous-dossier spÃ©cifique

-- â‘¡ Inspecter le contenu brut d'un fichier (sans charger)
SELECT $1, $2, $3, $4, $5
FROM @gcs_stage/HealthApp_2k.log
    (FILE_FORMAT => (TYPE = 'CSV' SKIP_HEADER = 1))
LIMIT 20;

-- â‘¢ Dry run complet â€” liste toutes les erreurs potentielles
COPY INTO RAW_EVENTS FROM @gcs_stage/HealthApp_2k.log
    FILE_FORMAT     = (FORMAT_NAME = 'csv_file')
    VALIDATION_MODE = 'RETURN_ALL_ERRORS';

-- â‘£ Historique des chargements avec erreurs
SELECT FILE_NAME, STATUS, ROW_COUNT, ERROR_COUNT, FIRST_ERROR, FIRST_ERROR_LINE
FROM TABLE(INFORMATION_SCHEMA.COPY_HISTORY(
    TABLE_NAME => 'RAW_EVENTS',
    START_TIME => DATEADD('HOURS', -24, CURRENT_TIMESTAMP())
))
WHERE STATUS = 'LOAD_FAILED' OR ERROR_COUNT > 0
ORDER BY LAST_LOAD_TIME DESC;

-- â‘¤ Statut d'un pipe Snowpipe
SELECT PARSE_JSON(SYSTEM$PIPE_STATUS('RAW.pipe_raw_events'));
-- â†’ clÃ©s : executionState, pendingFileCount, lastIngestedTimestamp

-- â‘¥ VÃ©rifier les droits du rÃ´le actif
SELECT CURRENT_ROLE();
SHOW GRANTS TO ROLE app_role;

-- â‘¦ Stages disponibles dans le schÃ©ma
SHOW STAGES IN SCHEMA HEALTH_APP.RAW;

-- â‘§ File Formats disponibles
SHOW FILE FORMATS IN SCHEMA HEALTH_APP.RAW;

-- â‘¨ VÃ©rifier l'intÃ©gration storage
DESC INTEGRATION gcs_int_ada;
-- â†’ STORAGE_GCP_SERVICE_ACCOUNT : email Ã  autoriser dans GCS
-- â†’ ENABLED : doit Ãªtre TRUE
```

---

## 13. Patterns RÃ©utilisables

### Pattern 1 â€” Chargement CSV depuis GCS (pipeline production complet)

```sql
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- SETUP INITIAL (Ã  exÃ©cuter une seule fois)
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- File Format
CREATE OR REPLACE FILE FORMAT mon_schema.csv_standard
    TYPE                         = CSV
    FIELD_DELIMITER              = ','
    SKIP_HEADER                  = 1
    FIELD_OPTIONALLY_ENCLOSED_BY = '"'
    NULL_IF                      = ('NULL', 'null', '', '\\N')
    EMPTY_FIELD_AS_NULL          = TRUE
    TRIM_SPACE                   = TRUE
    DATE_FORMAT                  = 'AUTO'       -- dÃ©tection auto du format date
    TIMESTAMP_FORMAT             = 'AUTO';      -- dÃ©tection auto du format timestamp

-- Storage Integration (ACCOUNTADMIN requis)
CREATE OR REPLACE STORAGE INTEGRATION gcs_int_prod
    TYPE                      = EXTERNAL_STAGE
    STORAGE_PROVIDER          = GCS
    ENABLED                   = TRUE
    STORAGE_ALLOWED_LOCATIONS = ('gcs://mon-bucket-prod/');
-- â†’ DESC INTEGRATION gcs_int_prod â†’ configurer IAM GCS avec l'email retournÃ©

-- Stage externe
CREATE OR REPLACE STAGE mon_schema.stage_gcs_prod
    URL                 = 'gcs://mon-bucket-prod/data/'
    STORAGE_INTEGRATION = gcs_int_prod
    FILE_FORMAT         = mon_schema.csv_standard;

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- CHARGEMENT RÃ‰CURRENT (pattern)
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
COPY INTO mon_schema.ma_table
FROM @mon_schema.stage_gcs_prod
PATTERN  = '.*YYYY-MM-DD.*\.csv'   -- adapter le pattern au nommage des fichiers
ON_ERROR = 'CONTINUE'
PURGE    = FALSE;

-- VÃ©rification post-chargement
SELECT COUNT(*) FROM mon_schema.ma_table;
SELECT FILE_NAME, STATUS, ROW_COUNT, ERROR_COUNT
FROM TABLE(INFORMATION_SCHEMA.COPY_HISTORY(
    TABLE_NAME => 'MA_TABLE',
    START_TIME => DATEADD('MINUTES', -10, CURRENT_TIMESTAMP())
));
```

---

### Pattern 2 â€” Snowpipe avec AUTO_INGEST GCS

```sql
-- File Format JSON
CREATE OR REPLACE FILE FORMAT mon_schema.fmt_json_events
    TYPE              = JSON
    STRIP_OUTER_ARRAY = TRUE;

-- Pipe
CREATE OR REPLACE PIPE mon_schema.pipe_events_auto
    AUTO_INGEST = TRUE
    COMMENT     = 'Ingestion continue events JSON depuis GCS'
AS
COPY INTO mon_schema.raw_events_json (data)
FROM (SELECT $1 FROM @mon_schema.stage_gcs_prod/events/)
FILE_FORMAT = (FORMAT_NAME = 'mon_schema.fmt_json_events')
ON_ERROR    = 'CONTINUE';

-- RÃ©cupÃ©rer notification_channel pour Pub/Sub
DESC PIPE mon_schema.pipe_events_auto;

-- Monitoring
SELECT PARSE_JSON(SYSTEM$PIPE_STATUS('mon_schema.pipe_events_auto'));
ALTER PIPE mon_schema.pipe_events_auto PAUSE;
ALTER PIPE mon_schema.pipe_events_auto RESUME;
```

---

### Pattern 3 â€” Chargement Parquet Data Lake avec mapping automatique

```sql
-- File Format Parquet
CREATE OR REPLACE FILE FORMAT mon_schema.fmt_parquet_dl
    TYPE               = PARQUET
    SNAPPY_COMPRESSION = TRUE
    BINARY_AS_TEXT     = FALSE;

-- Stage Data Lake
CREATE OR REPLACE STAGE mon_schema.stage_datalake
    URL                 = 'gcs://mon-datalake/tables/'
    STORAGE_INTEGRATION = gcs_int_prod
    FILE_FORMAT         = mon_schema.fmt_parquet_dl;

-- Chargement avec mapping automatique par nom de colonne
COPY INTO mon_schema.ma_table_analytique
FROM @mon_schema.stage_datalake/ma_table/
FILE_FORMAT          = (TYPE = 'PARQUET')
MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE  -- mapping colonne par nom
PATTERN              = '.*\.parquet'
ON_ERROR             = 'CONTINUE';
```

---

### Pattern 4 â€” Monitoring standard (copier-coller)

```sql
-- â”€â”€ Historique chargements 24h â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SELECT
    FILE_NAME,
    STATUS,
    ROW_COUNT,
    ERROR_COUNT,
    FIRST_ERROR,
    LAST_LOAD_TIME
FROM TABLE(INFORMATION_SCHEMA.COPY_HISTORY(
    TABLE_NAME => 'MA_TABLE',
    START_TIME => DATEADD('HOURS', -24, CURRENT_TIMESTAMP())
))
ORDER BY LAST_LOAD_TIME DESC;

-- â”€â”€ Taux de succÃ¨s sur 7 jours â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SELECT
    DATE_TRUNC('day', LAST_LOAD_TIME)   AS jour,
    COUNT(*)                            AS total_fichiers,
    SUM(CASE WHEN STATUS = 'LOADED'          THEN 1 ELSE 0 END) AS succes,
    SUM(CASE WHEN STATUS = 'LOAD_FAILED'     THEN 1 ELSE 0 END) AS echecs,
    SUM(ROW_COUNT)                      AS total_lignes,
    SUM(ERROR_COUNT)                    AS total_erreurs
FROM TABLE(INFORMATION_SCHEMA.COPY_HISTORY(
    TABLE_NAME => 'MA_TABLE',
    START_TIME => DATEADD('DAYS', -7, CURRENT_TIMESTAMP())
))
GROUP BY 1
ORDER BY 1 DESC;

-- â”€â”€ Fichiers jamais chargÃ©s (dans stage mais pas dans COPY_HISTORY) â”€â”€
-- Utile pour dÃ©tecter des fichiers oubliÃ©s
SELECT s.name AS stage_file
FROM TABLE(RESULT_SCAN(LAST_QUERY_ID())) s  -- rÃ©sultat du dernier LIST @stage
LEFT JOIN TABLE(INFORMATION_SCHEMA.COPY_HISTORY(
    TABLE_NAME => 'MA_TABLE',
    START_TIME => DATEADD('DAYS', -30, CURRENT_TIMESTAMP())
)) h ON h.FILE_NAME LIKE '%' || s.name
WHERE h.FILE_NAME IS NULL;
```

---

## 14. âœ… Checklist & Bonnes Pratiques

### PrÃ©requis avant tout chargement

```
[ ] Contexte dÃ©fini :
    [ ] USE ROLE mon_role;
    [ ] USE DATABASE HEALTH_APP;
    [ ] USE SCHEMA RAW;
    [ ] USE WAREHOUSE COMPUTE_WH;

[ ] Table de destination crÃ©Ã©e et schÃ©ma vÃ©rifiÃ©

[ ] File Format nommÃ© crÃ©Ã© (pas inline) :
    [ ] Type correct (CSV / JSON / Parquet)
    [ ] FIELD_DELIMITER adaptÃ© au fichier
    [ ] SKIP_HEADER = 1 si en-tÃªtes prÃ©sents
    [ ] NULL_IF et DATE_FORMAT testÃ©s

[ ] Stage accessible :
    [ ] LIST @mon_stage; retourne des fichiers
    [ ] Permissions IAM configurÃ©es (Storage Integration)

[ ] Droits du rÃ´le vÃ©rifiÃ©s :
    [ ] USAGE ON STAGE
    [ ] INSERT ON TABLE
    [ ] USAGE ON WAREHOUSE
```

### Ordre recommandÃ© pour un premier chargement

```mermaid
graph TD
    S1["1. CrÃ©er Database / Schema / Table"] --> S2
    S2["2. CrÃ©er le File Format nommÃ©"] --> S3
    S3["3. CrÃ©er la Storage Integration\n(ACCOUNTADMIN)"] --> S4
    S4["4. Configurer IAM dans GCS/S3/Azure\navec l'email DESC INTEGRATION"] --> S5
    S5["5. CrÃ©er le Stage externe"] --> S6
    S6["6. LIST @stage â€” vÃ©rifier accÃ¨s"] --> S7
    S7["7. Inspecter le fichier brut\nSELECT $1,$2 FROM @stage LIMIT 5"] --> S8
    S8["8. VALIDATION_MODE = 'RETURN_ALL_ERRORS'\n(dry run)"] --> S9
    S9{"Erreurs ?"}
    S9 -->|"Oui"| S8b["Corriger le File Format\nou le fichier source"]
    S8b --> S8
    S9 -->|"Non"| S10["9. COPY INTO rÃ©el\n(ON_ERROR = 'CONTINUE' pour 1er run)"]
    S10 --> S11["10. VÃ©rifier COUNT(*) + COPY_HISTORY"]

    style S9 fill:#fdcb6e,color:#333
    style S10 fill:#00b894,color:#fff
    style S11 fill:#0984e3,color:#fff
```

### Pendant le chargement

- [ ] `ON_ERROR = 'CONTINUE'` en phase de dÃ©couverte, `'ABORT_STATEMENT'` en production critique
- [ ] `PURGE = FALSE` par dÃ©faut â€” conserver les fichiers pour audit
- [ ] Ne jamais utiliser `FORCE = TRUE` en production (doublons possibles)
- [ ] Partitionner les gros fichiers en chunks de **100â€“250 MB** pour la parallÃ©lisation optimale

### AprÃ¨s le chargement

- [ ] VÃ©rifier `COUNT(*)` et comparer avec le nombre de lignes attendu
- [ ] ContrÃ´ler `COPY_HISTORY` â€” `ERROR_COUNT` doit Ãªtre 0 ou acceptable
- [ ] VÃ©rifier un Ã©chantillon (`SELECT * LIMIT 100`) : types corrects, nulls cohÃ©rents
- [ ] Nettoyer le stage si `PURGE = FALSE` : `REMOVE @mon_stage/fichier.csv;`
- [ ] Documenter le File Format utilisÃ© dans un registre d'objets Snowflake

### Bonnes pratiques gÃ©nÃ©rales

- [ ] **Toujours crÃ©er un `FILE FORMAT` nommÃ©** â€” jamais inline dans COPY INTO
- [ ] **Utiliser des `STORAGE INTEGRATION`** â€” ne jamais mettre de clÃ©s AWS/GCS en dur dans le SQL
- [ ] **Nommer les objets clairement** : `stage_gcs_health_prod`, `csv_health_events`
- [ ] **Pour les chargements rÃ©currents** : encapsuler dans une ProcÃ©dure StockÃ©e appelÃ©e par une Task
- [ ] **Monitorer Snowpipe** avec `SYSTEM$PIPE_STATUS` â€” crÃ©er des alertes sur `pendingFileCount`
- [ ] **Tester le File Format** sur un Ã©chantillon de 100-1000 lignes avant de charger millions de lignes

---

## 15. Glossaire Complet

| Terme | DÃ©finition |
|---|---|
| **Stage** | Zone de transit Snowflake oÃ¹ les fichiers sont dÃ©posÃ©s avant d'Ãªtre chargÃ©s dans une table |
| **Internal Stage** | Stage gÃ©rÃ© par Snowflake (stockage interne) : `@~`, `@%table`, `@named` |
| **External Stage** | Stage pointant vers un stockage cloud externe (GCS, S3, Azure) sans copier les fichiers |
| **Storage Integration** | Objet Snowflake contenant les credentials IAM sÃ©curisÃ©s pour accÃ©der Ã  un bucket cloud |
| **File Format** | Objet dÃ©crivant comment parser un fichier (sÃ©parateur, encoding, format datesâ€¦) |
| **COPY INTO** | Commande principale de chargement en masse depuis un stage vers une table |
| **Snowpipe** | MÃ©canisme d'ingestion continue (serverless) dÃ©clenchÃ© par Ã©vÃ©nement cloud |
| **PUT** | Commande SnowSQL CLI pour uploader un fichier local vers un stage interne |
| **GET** | Commande SnowSQL CLI pour tÃ©lÃ©charger un fichier depuis un stage interne |
| **VALIDATION_MODE** | Option COPY INTO pour tester le chargement sans insÃ©rer de donnÃ©es (dry run) |
| **ON_ERROR** | Comportement en cas d'erreur : `ABORT_STATEMENT`, `CONTINUE`, `SKIP_FILE` |
| **PURGE** | Option COPY INTO : `TRUE` = supprimer le fichier du stage aprÃ¨s chargement |
| **FORCE** | Option COPY INTO : `TRUE` = recharger mÃªme si dÃ©jÃ  traitÃ© (âš ï¸ doublons) |
| **DÃ©duplication** | MÃ©canisme natif Snowflake : empÃªche de recharger deux fois le mÃªme fichier |
| **VARIANT** | Type de donnÃ©es Snowflake pour stocker du JSON/AVRO/XML semi-structurÃ© |
| **STRIP_OUTER_ARRAY** | Option JSON File Format : dÃ©structure `[{...},{...}]` en lignes sÃ©parÃ©es |
| **MATCH_BY_COLUMN_NAME** | Option COPY INTO Parquet : mapping des colonnes par nom au lieu de position |
| **COPY_HISTORY** | Vue `INFORMATION_SCHEMA` traÃ§ant tous les chargements des 30 derniers jours |
| **AUTO_INGEST** | Option Snowpipe : dÃ©clenche le chargement automatiquement via notifications cloud |
| **notification_channel** | ARN Pub/Sub/SQS Ã  configurer dans le cloud pour dÃ©clencher `AUTO_INGEST` |
| **SYSTEM$PIPE_STATUS** | Fonction systÃ¨me retournant le statut complet d'un pipe (JSON) |
| **LIST** | Commande listant les fichiers prÃ©sents dans un stage |
| **REMOVE** | Commande supprimant des fichiers d'un stage interne |
| `$1`, `$2`â€¦ | RÃ©fÃ©rences positionnelles aux colonnes d'un fichier brut dans un stage |
| **IDENTIFIER()** | Permet d'utiliser une variable STRING comme nom de table dynamique dans les procÃ©dures |
| **TRY_TO_DATE()** | Variante de TO_DATE() qui retourne NULL au lieu d'une erreur si le format est invalide |
| **SQLERRM** | Variable de contexte contenant le message de la derniÃ¨re erreur (dans un bloc EXCEPTION) |
| **SQLROWCOUNT** | Variable contenant le nombre de lignes affectÃ©es par le dernier DML |

---

## ğŸ“š Ressources Officielles

| Sujet | Lien |
|---|---|
| Vue d'ensemble chargement | [data-load-overview](https://docs.snowflake.com/fr/user-guide/data-load-overview) |
| ConsidÃ©rations prÃ©paration | [data-load-considerations-prepare](https://docs.snowflake.com/fr/user-guide/data-load-considerations-prepare) |
| CREATE DATABASE | [create-database](https://docs.snowflake.com/fr/sql-reference/sql/create-database) |
| CREATE SCHEMA | [create-schema](https://docs.snowflake.com/fr/sql-reference/sql/create-schema) |
| CREATE TABLE | [create-table](https://docs.snowflake.com/fr/sql-reference/sql/create-table) |
| Types de donnÃ©es | [sql-reference-data-types](https://docs.snowflake.com/fr/sql-reference-data-types) |
| CREATE STAGE | [create-stage](https://docs.snowflake.com/fr/sql-reference/sql/create-stage) |
| CREATE FILE FORMAT | [create-file-format](https://docs.snowflake.com/fr/sql-reference/sql/create-file-format) |
| COPY INTO \<table\> | [copy-into-table](https://docs.snowflake.com/fr/sql-reference/sql/copy-into-table) |
| COPY_HISTORY | [functions/copy_history](https://docs.snowflake.com/fr/sql-reference/functions/copy_history) |
| INSERT | [insert](https://docs.snowflake.com/fr/sql-reference/sql/insert) |
| PUT | [sql/put](https://docs.snowflake.com/fr/sql-reference/sql/put) |
| GET | [sql/get](https://docs.snowflake.com/fr/sql-reference/sql/get) |
| CREATE STORAGE INTEGRATION | [create-storage-integration](https://docs.snowflake.com/fr/sql-reference/sql/create-storage-integration) |
| Configurer GCS IAM | [data-load-gcs-config](https://docs.snowflake.com/fr/user-guide/data-load-gcs-config) |
| Snowpipe Introduction | [data-load-snowpipe-intro](https://docs.snowflake.com/fr/user-guide/data-load-snowpipe-intro) |
| Snowpipe GCS AUTO_INGEST | [data-load-snowpipe-auto-gcs](https://docs.snowflake.com/fr/user-guide/data-load-snowpipe-auto-gcs) |
| Snowpipe Streaming | [snowpipe-streaming-overview](https://docs.snowflake.com/fr/user-guide/snowpipe-streaming/data-load-snowpipe-streaming-overview) |
| Troubleshooting chargement | [data-load-troubleshooting](https://docs.snowflake.com/fr/user-guide/data-load-troubleshooting) |
| Semi-structured data | [semistructured-intro](https://docs.snowflake.com/fr/user-guide/semistructured-intro) |
| Python Connector | [python-connector-example](https://docs.snowflake.com/fr/user-guide/python-connector-example) |

---

## ğŸ¯ Les 5 Principes Ã  Retenir

```mermaid
mindmap
  root((Chargement\nSnowflake))
    Architecture
      Source Stage Table
      Internal vs External Stage
      Storage Integration = sÃ©curitÃ© IAM
      Jamais de clÃ©s en dur dans le SQL
    File Format
      Toujours nommÃ© jamais inline
      Tester sur echantillon dabord
      VALIDATION_MODE dry run
      DATE_FORMAT et NULL_IF critiques
    COPY INTO
      ON_ERROR CONTINUE pour decouverte
      ABORT_STATEMENT en production
      FORCE FALSE evite les doublons
      Chunks 100-250MB pour parallelisme
    Snowpipe
      Serverless facturation a la seconde
      AUTO_INGEST via notification cloud
      Monitorer PIPE_STATUS regulierement
      PAUSED puis RESUME si incident
    Diagnostic
      LIST pour verifier acces stage
      SELECT dollar1 inspecter fichier brut
      COPY_HISTORY historique complet
      VALIDATION_MODE avant premier run
```

---

*â„ï¸ Snowflake Data Loading â€” Guide Professionnel | Projet health_app | docs.snowflake.com/fr*
